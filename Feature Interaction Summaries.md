# Feature Interaction

## List

- [x] Predictive learning via rule ensembles
- [x] Detecting statistical interactions with additive groves of trees
- [x] Accurate intelligible models with pairwise interactions
Michael Tsang
- [x] NID：Detecting Statistical Interactions from Neural Network Weights
- [x] NIT：Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability
- [x] Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection
- [x] Learning Global Pairwise Interactions with Bayesian Neural Networks
FM相关
- [x] FM：Factorization machines
- [x] FFM：Field-aware Factorization Machines for CTR Prediction
- [x] DeepFM：DeepFM: A Factorization-Machine based Neural Network for CTR Prediction
- [x] xDeepFM：xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems
- [x] Wide & Deep Learning for Recommender Systems
- [x] Deep & Cross：Deep & Cross Network for Ad Click Predictions
- [x] AFM：Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks
- [x] DIN：Deep Interest Network for Click-Through Rate Prediction
- [x] AutoInt: Automatic feature interaction learning via self-attentive neural networks

## Predictive learning via rule ensembles

> ensemble: rules + linear (robust), penalized, rule by trees (correlated predictor control by edge w/ incentive).
>
> Loss Function: regression and classification
>
> Importance: of a rule and of a predictor.

Interaction: Partial dependence and statistic. Bootstrap to generate a reference distribution.

> $F_{s}\left(\mathbf{x}_{s}\right)=E_{\mathbf{x}_{\backslash s}}\left[F\left(\mathbf{x}_{s}, \mathbf{x}_{\backslash s}\right)\right]$
>
> $H_{j k}^{2}=\sum_{i=1}^{N}\left[\hat{F}_{j k}\left(x_{i j}, x_{i k}\right)-\hat{F}_{j}\left(x_{i j}\right)-\hat{F}_{k}\left(x_{i k}\right)\right]^{2} / \sum_{i=1}^{N} \hat{F}_{j k}^{2}\left(x_{i j}, x_{i k}\right)$
>
> $H_{j}^{2}=\sum_{i=1}^{N}\left[F\left(\mathbf{x}_{i}\right)-\hat{F}_{j}\left(x_{i j}\right)-\hat{F}_{\backslash j}\left(\mathbf{x}_{i \backslash j}\right)\right]^{2} / \sum_{i=1}^{N} F^{2}\left(\mathbf{x}_{i}\right)$
>
> $\begin{aligned} H_{j k l}^{2}=\sum_{i=1}^{N}\left[\hat{F}_{j k l}\left(x_{i j}, x_{i k}, x_{i l}\right)-\hat{F}_{j k}\left(x_{i j}, x_{i k}\right)\right.& \\-\hat{F}_{j l}\left(x_{i j}, x_{i l}\right)-\hat{F}_{k l}\left(x_{i k}, x_{i l}\right)+\hat{F}_{j}\left(x_{i j}\right) & \\\left.+\hat{F}_{k}\left(x_{i k}\right)+\hat{F}_{l}\left(x_{i l}\right)\right]^{2} / \sum_{i=1}^{N} \hat{F}_{j k l}^{2}\left(x_{i j}, x_{i k}, x_{i l}\right) \end{aligned}$



## **Accurate Intelligible Models with Pairwise Interactions**

### GA$^2$M

Greedy, Fisrst fit GAM residual with a subset of pairs $\mathcal S$, then fit the new residual $R$ respectively with each remaining pair, add the best fit pair to $\mathcal S$, and iterate.

### FastInteractionDetection

**FAST**: given bins, first compute histogram and cumulative histogram on target and weight to form a lookup table, then reuse them to calculate sum of target on 4 quadrants for each cut. RSS is easily calculable for bin fctns.



## Detecting statistical interactions with additive groves of trees

### Summary

* defines an intereaction statistic by complete model and interaction-free model. significant if greater than 3*std, std generated by resampling. need to remove correlated.

* uses an additive tree model with a 3-layer algm bagging-growing-backfitting

* restrict an inteaction-free tree groove model by forbidding one of the variables each time and selecting the best for each tree generation. 

  > **Que: why this prevents just using the more important var comparing to a ban-same-brance procedure?**

* beats the *Predictive learning via rule ensembles* statistic in avoiding spurious interaction at sparse regions.



## NID：Detecting Statistical Interactions from Neural Network Weights

### Summary

Consider how a neuron network captures interaction. 1) common node (large weight) in the first hidden layer. 2) the node passes to the result (product of weight matrices) 1) and 2) leads to a measure of interaction of all orders.

Train a neuron net, then use a greedy algrithm to rank potential interaction of all orders, then find a cutoff on a vaidation set, adding the interaction terms to a GAM until performance plateaus out.



## NIT：Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability

### Summary

Neuron network with a gate on the weight matrix from the input to the first hidden layer that separates the decendent network into several disjoint sub-networks, yielding a GAM with interaction terms. The maximum order of interaction and each order of interation are penalized in regularization term.





## FM：Factorization machines

### Summary

FM uses a more restricted setting of parameteriation for interaction terms by dot product of rows of a matrix. This works well for hugely sparse categorical data (esp. w/ one-hot coding) in that it generalizes for interactions that did not show up in the dataset. It has linear complexity for computation $O(k\,n)$. 

In terms of interaction, it only considers $\prod_{i=1}^lx_{j_i}$, and only considers $x_i$ for main effect. This is complete for categorical data. ~~This is okay, since FM just covers categorical data for which the uncertainty of encoding makes further transformation senseless.~~

**embedding, parameter sharing.**



## Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection

### Summary

Given a **black-box model** (e.g. a recommendation system), we want to detect the global interactions. For each instance, purturb with *LIME* to get a local sample and calculate the predictions with the black-box model. Detect on this local dataset the **local** interaction with *NID*. Count interaction and get those local interactions that appear consistantly many times and call it a **global** interaction.



## Learning Global Pairwise Interactions with Bayesian Neural Networks

### Summary

Defines an interaction score by exploiting the notion of hessian of the model. First clustering the datapoints. On each cluster aggregates the hessian and takes absolute value, and then does a weighed average for all clusters.

Uses a Bayesian Neuron Network for the model of which the hessian is computed. The score is thus a random variable with a known distribution of the model weights, and need to use Monte Carlo estimators for mean and variance. Confidence interval can be obtained and hypothesis tests can also be conducted.



## AutoInt: Automatic feature interaction learning via self-attentive neural networks

### Summary

This model is a neuron network with a novel multi-head self-attention interaction layer as hidden layers. the recoded (sparse) features are first mapped into a different vector space, and operated by nonlinear functions to obtain an "interaction score" (weight). This weights and all the features are used to calculate a vector for the next layer or output (through a GLM). The weights can be used to measure interaction strengths, however, this model is black-box and there is no separation into of additive interaction terms even within a layer. everything is entangled. This model outperforms others and is a state-of-art for CTR detection.



## FFM：Field-aware Factorization Machines for CTR Prediction

### Summary

To solve CTR problems with sparse data, this method modifies FM in that 1. each feature is assigned to a field. 2. the latent vector learned in FM is replaced with multiple vectors, each corresponding to one of the fields. 3. when computing the coefficient of the interaction term, inner product is taken with the vector corresponding to the field of the other feature.



## Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks  

### Summary

FM is adjusted by replacing the coefficient of the interaction term. instead of a dot product of the embedding vectors, use their hadamard product weight-averaged by  an "interaction importance/score". The weight is learned by an attention network (with ReLU), to omit the non-important interactions and focus on important ones which are rare in sparse data.



## Wide & Deep Learning for Recommender Systems

### Summary

To deal with sparse data for a recommendation system, a wide model — a GLM with feature crossing and a deep model — a neuron network on the dense embedding vectors of features are jointly trained. 

The deep part is good at generalization but could over-generalize and produce spurious interaction despite the rarity of interaction in sparse data. the wide part fixes this by explicitly memorizing "rules". the deep part fixes the wide part which is not good at generalizing to unprecedented instances as the data is sparse.

in terms of interaction detection, the wide part does so by feature crossing, but the deep part is a black box.

problem: cross features in the wide part still needs feature engineering.

##Deep & Cross Network for Ad Click Predictions

embedding matrix can be learned in the model

in each layer the crossed vector $\mathbf x_l$ is weight-crossed with $\mathbf x_0$, and added to itself $\mathbf x_l$ and a bias.

### Theoretical explanations of Cross network

### Summary

In parallel to a DNN, a cross network is jointly trained. in each layer of the cross network, interaction is learned by weigted crossing with the original vector from the input (dense numerical features concatenated with embedded sparse categorical features.) it outperforms state-of-art DNN models, but interaction is in a black-box and no explicit representation (i.e. interaction detection such as weighted addition) could be provided.

According to the *xDeepFM* paper, the cross network limits the form of interaction as a scalar multiple of $x_0$, and the interaction is bit-wise.





## DeepFM：DeepFM: A Factorization-Machine based Neural Network for CTR Prediction

compared with wide & deep, no manual interactoin needs to be designed, and a common vector allows better efficiency.

### Summary

To predict CTR, this model is modified from the Wide & Deep by replacing the GLM with an FM, and having the jointly trained FM and  NN share the same feature embedding vector, allowing low- and high- order interactions to be learned from raw features. (The embedding map is just $V^T$, where the $i$th row of $V$, $V_i$ is the latent vector corresponding to $x_i$ learned in FM).



## xDeepFM：xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems

bit-wise vs vector-wise interaction

### Summary

The model comprises a linear model for main effects, a Compressed Interaction Network for explicit interaction modeling, and a DNN for implicit interaction modeling. In comparison to FM, the CIN uses vector interaction instead of bit-wise interaction. ~~the CIN structure is equivalent to a CNN with filter and pooling.~~ the advantage is that each layer goes into the polling as an additive term, allowing each order of interaction to be modeled. 

~~no explicit interaction detection of specific features is considered, but it is a result of product of weights, as is equivalent to a polynomial.~~ 

the model is slow and seemingly has an unnecessary redundancy that is merged?

~~The network without nonlinear activation is equivalent to polynomial regression.~~



## Deep Interest Network for Click-Through Rate Prediction

CTR prediction

### Summary

To address to the CTR prediction problem with sparse features, this model consists of two parts: feature embedding and MLP. the novelty of this model is the embedding map. for mult-hot features, such as the click history of many goods, instead of traditional sum/average pooling before embedding, this model uses a pooling weight learned by a local "activation unit" — an attention network. This could leverage the user preference history and identify the true interest from diverse interests.

Besides, training is optimized by a novel mini-batch regularization and a distribution-specific activation function.

This paper seems not explicitly relevant to feature interaction detection, except for its value in sparse feature embedding as a preprocessing trick.

